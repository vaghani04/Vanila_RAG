[
    "The Transformer model is a neural network architecture that uses solely attention mechanisms, without recurrence or convolutions. It shows superior performance in machine translation tasks, including English-to-German and English-to-French translation. The Transformer model is also applicable to other tasks, such as English constituency parsing.",
    "Recurrent neural networks (RNNs) are state-of-the-art in sequence modeling but are inherently sequential, limiting parallelization during training. Attention mechanisms, often used with RNNs, model dependencies without regard to distance. The Transformer model proposes eschewing recurrence and relying solely on an attention mechanism, allowing for more parallelization and achieving state-of-the-art translation quality with minimal training time.",
    "The Transformer model uses self-attention to compute input and output representations, unlike previous transduction models that rely on RNNs or convolutions. Self-attention relates different positions within a sequence to compute a representation of the sequence, and has been successfully used in various tasks.",
    "The Transformer model has an encoder-decoder structure where the encoder maps an input sequence to a sequence of continuous representations, and the decoder generates an output sequence one element at a time. The encoder and decoder stacks consist of identical layers with self-attention mechanisms and fully connected feed-forward networks. Residual connections and layer normalization are employed around each sub-layer. The decoder includes an additional sub-layer for multi-head attention over the encoder output. Masking is used in the decoder to prevent positions from attending to subsequent positions.",
    "Scaled Dot-Product Attention: attention mechanism that takes queries, keys, and values as input and outputs a weighted sum of values, with weights determined by a compatibility function of query and key. Dot-product attention is commonly used, with additive attention as an alternative. Dot-product attention is faster and more space-efficient, while additive attention may perform better for larger values of dk.",
    "Multi-head attention uses multiple attention heads with different linear projections to attend to information from different representation subspaces. Each head computes attention in parallel, and the results are concatenated and projected to produce the final output. This allows the model to attend to different aspects of the input simultaneously, similar to how humans can focus on multiple aspects of a scene at once.",
    "The Transformer uses multi-head attention in three ways: encoder-decoder attention, encoder self-attention, and masked decoder self-attention to allow each position to attend to all positions in the previous layer within the encoder or decoder, and to attend to all positions in the input sequence (encoder-decoder attention only).",
    "A feed-forward network (FFN) is used in each layer of the encoder and decoder, consisting of two linear transformations with a ReLU activation in between. The linear transformations are position-wise, meaning they are applied to each position separately and identically, and they use different parameters from layer to layer. The dimensionality of the input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.",
    "To enable the model to utilize the order of sequences, positional encodings are added to input embeddings. There are many options for positional encodings, both learned and fixed. This work uses sine and cosine functions of different frequencies. Alternatively, learned positional embeddings can be used, yielding similar results. The sinusoidal version was chosen because it may allow the model to extrapolate to longer sequence lengths than encountered during training.",
    "Long-range dependency learning is facilitated by short path length in networks, with self-attention layers having a constant path length compared to recurrent layers' O(n) sequential operations. Though recurrent layers are faster than self-attention layers with long sequences and high-dimensional representations, self-attention layers have the advantage of parallelizability and potential for improved interpretability.",
    "**Training Summary:**\n\n* **Data:** WMT 2014 English-German/French datasets (4.5M/36M sentence pairs)\n* **Batching:** 25000 source/target tokens per batch\n* **Hardware:** 8 NVIDIA P100 GPUs\n* **Training Time:** 12 hours (base models), 3.5 days (big models)\n* **Optimizer:** Adam with specific hyperparameters\n* **Learning Rate:** Varied according to formula, linear warmup followed by inverse square root decay",
    "The Transformer model outperforms previous state-of-the-art models in BLEU scores for English-to-German and English-to-French translation tasks, while requiring less training cost.\n\nRegularization techniques used during Transformer training include residual dropout, label smoothing, and attention dropout. Residual dropout is applied to each sub-layer's output, while label smoothing is used with a value of ϵls = 0.1. Attention dropout is used with a probability of 0.1 for the base model.",
    "On the WMT 2014 English-to-German translation task, the Transformer (big) model achieves a BLEU score of 28.4, surpassing all previous models, including ensembles. On the English-to-French task, it achieves a BLEU score of 41.0, again outperforming previous models at a lower training cost. The base models use single models averaged from the last 5 checkpoints, while the big models use averages from the last 20 checkpoints. Beam search with a beam size of 4 and length penalty of 0.6 is used. The maximum output length during inference is set to input length + 50, with early termination used when possible.",
    "The table shows variations on the Transformer architecture and their impact on English-to-German translation performance. Variation A: Varying attention heads and key/value dimensions shows that single-head attention is worse, while too many heads also decrease quality. Variation B: Reducing attention key size hurts model quality. Variations C and D: Larger models are better and dropout helps avoid overfitting. Variation E: Learned positional embeddings give comparable results to sinusoidal encoding.",
    "The Transformer model performs well in English constituency parsing, as shown in Table 4. Despite no task-specific tuning, the Transformer outperforms all previously reported models except the Recurrent Neural Network Grammar, and outperforms the Berkeley-Parser even when training only on a small dataset.",
    "Transformer model using attention replaces recurrent layers in encoder-decoder architectures. It trains faster than recurrent or convolutional models and achieves state-of-the-art performance in English-to-German and English-to-French translation tasks. Future research will explore applications to other tasks, modalities, and making generation less sequential.",
    "1. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n2. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n3. Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\n4. Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\n5. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n6. Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\n7. Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n8. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\n9. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n10. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n11. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n12. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n13. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n14. Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\n15. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n16. Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\n17. Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\n18. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\n19. Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\n20. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n21. Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\n22. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n23. Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.",
    "Various research papers on neural machine translation, corpus annotation, self-training for parsing, attention models, abstractive summarization, tree annotation, language models, neural machine translation of rare words, neural networks, end-to-end memory networks, sequence to sequence learning, inception architecture, grammar as a foreign language, Google neural machine translation system, deep recurrent models for neural machine translation, and shift-reduce constituent parsing.",
    "Figure 3: Attention mechanism following long-distance dependencies in the encoder self-attention. Many attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’.\n\nFigure 4: Two attention heads involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6.\n\nFigure 5: Many attention heads exhibit behaviour related to the structure of the sentence. The heads learned to perform different tasks.",
    "| Item Name | Category | Total Benefit | Total No |\n|---|---|---|---|\n| $546,654,013 |  | $321,651 | $32,245 |\n| $503,210,012 |  | $10,340 | $654,321 |\n| $32,650,214 |  | $32,650,214 |  |"
]